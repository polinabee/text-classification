{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt\n",
    "Use the following dataset\n",
    "https://github.com/microsoft/ML-Server-Python-Samples/blob/master/microsoftml/202/data/sentiment_analysis/yelp_labelled.txt\n",
    "to classify reviews to positive sentiment reviews or negative sentiment reviews.\n",
    "Train a RNN and a convolutional network to this task. You should train them character-wise and word-wise (one model for each).\n",
    "Complete the whole assignment in a single self-contained notebook/colab. Find a good architecture for each model and tune the model parameters.\n",
    "Compare each methods accuracy.\n",
    "Which one gives you the best results?\n",
    "Write 1-2 paragraphs on the motivation and the way you came about in the design of the models and your observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import data\n",
    "- 0 for positive, 1 for negative, tab delimited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import imdb \n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Flatten \n",
    "from tensorflow.keras.layers import LSTM \n",
    "from tensorflow.keras.layers import Embedding \n",
    "from tensorflow.keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>I think food should have flavor and texture an...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Appetite instantly gone.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Overall I was not impressed and would not go b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>The whole experience was underwhelming, and I ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Then, as if I hadn't wasted enough of my life ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  label\n",
       "0                             Wow... Loved this place.      1\n",
       "1                                   Crust is not good.      0\n",
       "2            Not tasty and the texture was just nasty.      0\n",
       "3    Stopped by during the late May bank holiday of...      1\n",
       "4    The selection on the menu was great and so wer...      1\n",
       "..                                                 ...    ...\n",
       "995  I think food should have flavor and texture an...      0\n",
       "996                           Appetite instantly gone.      0\n",
       "997  Overall I was not impressed and would not go b...      0\n",
       "998  The whole experience was underwhelming, and I ...      0\n",
       "999  Then, as if I hadn't wasted enough of my life ...      0\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_labelled = pd.read_csv('yelp_labelled.txt', sep='\\t',header=None)\n",
    "yelp_labelled.columns = ['text','label']\n",
    "yelp_labelled\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Step 2: Word 2 Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "positive_posts = pd.Series.to_numpy(yelp_labelled[yelp_labelled.label == 1]['text'])\n",
    "negative_posts = pd.Series.to_numpy(yelp_labelled[yelp_labelled.label == 0]['text'])\n",
    "print(len(positive_posts))\n",
    "print(len(negative_posts))\n",
    "posts = positive_posts + negative_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2VecVocab at 0x153db44f0>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v = Word2Vec(size=100, min_count=1)\n",
    "w2v.build_vocab(map(lambda x: x.split(), posts), )\n",
    "w2v.vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78274, 583160)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.train(posts, total_examples=w2v.corpus_count,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('groups', 0.34418344497680664),\n",
       " ('Luke', 0.3424365818500519),\n",
       " ('go!Host', 0.33621126413345337),\n",
       " ('orders', 0.33029842376708984),\n",
       " ('eggs', 0.32191407680511475),\n",
       " ('amazing...rge', 0.3136864900588989),\n",
       " ('disappointed!', 0.29802176356315613),\n",
       " ('deeply', 0.2806062698364258),\n",
       " ('company', 0.2769371271133423),\n",
       " ('Steve', 0.2752118706703186)]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.most_similar(positive=['crust'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.18840203"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.similarity('I', 'you')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05288045"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.similarity('flavor', 'food')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN with Words\n",
    "using this tutorial:\n",
    "https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/ <br>\n",
    "and the \"LSTM Classification Generation\" notebook<br>\n",
    "From wikipedia: \"Long short-term memory (LSTM) is an artificial recurrent neural network (RNN)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.preprocessing.text import one_hot, text_to_word_sequence\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM, GRU\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filtered_positive_posts = list(filter(lambda p: len(p) > 0, positive_posts))\n",
    "filtered_negative_posts = list(filter(lambda p: len(p) > 0, negative_posts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# text processing - one hot builds index of the words\n",
    "pos_one_hot = []\n",
    "neg_one_hot = []\n",
    "n = 30000\n",
    "for post in filtered_positive_posts:\n",
    "    try:\n",
    "        pos_one_hot.append(one_hot(post, n, split=\" \", filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True))\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "for post in filtered_negative_posts:\n",
    "    try:\n",
    "        neg_one_hot.append(one_hot(post,n,split=\" \",filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~',lower=True))\n",
    "    except:\n",
    "        continue"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 0 for bad, 1 for good\n",
    "concatenate_array_rnn = np.concatenate((np.zeros(len(neg_one_hot)),\n",
    "                                        np.ones(len(pos_one_hot))))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_rnn, X_test_rnn, y_train_rnn, y_test_rnn = train_test_split(np.concatenate((neg_one_hot,pos_one_hot)),\n",
    "                                                                    concatenate_array_rnn, \n",
    "                                                                    test_size=0.2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get max length review\n",
    "maxlen = max([len(r) for r in pd.Series.tolist(yelp_labelled.text)])\n",
    "X_train_rnn = sequence.pad_sequences(X_train_rnn, maxlen=maxlen)\n",
    "X_test_rnn = sequence.pad_sequences(X_test_rnn, maxlen=maxlen)\n",
    "print('X_train_rnn shape:', X_train_rnn.shape, y_train_rnn.shape)\n",
    "print('X_test_rnn shape:', X_test_rnn.shape, y_test_rnn.shape)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "max_features = 30000\n",
    "dimension = 128\n",
    "output_dimension = 128\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, dimension))\n",
    "model.add(LSTM(output_dimension))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error', optimizer='sgd', metrics=['accuracy'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.fit(X_train_rnn, y_train_rnn, batch_size=32,\n",
    "          epochs=4, validation_data=(X_test_rnn, y_test_rnn))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "score, acc = model.evaluate(X_test_rnn, y_test_rnn, batch_size=32)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Using TFIDF Vectorizer as input instead of one hot"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(decode_error='ignore', norm='l2', min_df=5)\n",
    "tfidf_good = vectorizer.fit_transform(filtered_positive_posts)\n",
    "tfidf_bad = vectorizer.fit_transform(filtered_negative_posts)\n",
    "\n",
    "flattened_array_tfidf_good = tfidf_good.toarray()\n",
    "flattened_array_tfidf_bad = tfidf_bad.toarray()\n",
    "\n",
    "#0 bad, 1 good\n",
    "y_rnn = np.concatenate((np.zeros(len(flattened_array_tfidf_bad)),\n",
    "                                        np.ones(len(flattened_array_tfidf_good))))\n",
    "\n",
    "X_train_rnn, X_test_rnn, y_train_rnn, y_test_rnn = train_test_split(np.concatenate((flattened_array_tfidf_male, \n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train_rnn = sequence.pad_sequences(X_train_rnn, maxlen=maxlen)\n",
    "X_test_rnn = sequence.pad_sequences(X_test_rnn, maxlen=maxlen)\n",
    "print('X_train_rnn shape:', X_train_rnn.shape, y_train_rnn.shape)\n",
    "print('X_test_rnn shape:', X_test_rnn.shape, y_test_rnn.shape)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "max_features = 30000\n",
    "dimension = 30\n",
    "output_dimension =20\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, dimension))\n",
    "model.add(LSTM(output_dimension))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='mean_squared_error',optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train_rnn, y_train_rnn, \n",
    "          batch_size=32, epochs=4,\n",
    "          validation_data=(X_test_rnn, y_test_rnn))\n",
    "\n",
    "score,acc = model.evaluate(X_test_rnn, y_test_rnn, \n",
    "                           batch_size=32)\n",
    "\n",
    "print(score, acc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 30000\n",
    "dimension = 30\n",
    "output_dimension =20\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, dimension))\n",
    "model.add(LSTM(output_dimension))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='mean_squared_error',optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train_rnn, y_train_rnn, \n",
    "          batch_size=32, epochs=4,\n",
    "          validation_data=(X_test_rnn, y_test_rnn))\n",
    "\n",
    "score,acc = model.evaluate(X_test_rnn, y_test_rnn, \n",
    "                           batch_size=32)\n",
    "\n",
    "print(score, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 for bad, 1 for good\n",
    "concatenate_array_rnn = np.concatenate((np.zeros(len(neg_one_hot)),\n",
    "                                        np.ones(len(pos_one_hot))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_rnn, X_test_rnn, y_train_rnn, y_test_rnn = train_test_split(np.concatenate((neg_one_hot,pos_one_hot)),\n",
    "                                                                    concatenate_array_rnn, \n",
    "                                                                    test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_rnn shape: (800, 149) (800,)\n",
      "X_test_rnn shape: (200, 149) (200,)\n"
     ]
    }
   ],
   "source": [
    "# get max length review\n",
    "maxlen = max([len(r) for r in pd.Series.tolist(yelp_labelled.text)])\n",
    "X_train_rnn = sequence.pad_sequences(X_train_rnn, maxlen=maxlen)\n",
    "X_test_rnn = sequence.pad_sequences(X_test_rnn, maxlen=maxlen)\n",
    "print('X_train_rnn shape:', X_train_rnn.shape, y_train_rnn.shape)\n",
    "print('X_test_rnn shape:', X_test_rnn.shape, y_test_rnn.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 30000\n",
    "dimension = 128\n",
    "output_dimension = 128\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, dimension))\n",
    "model.add(LSTM(output_dimension))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "25/25 [==============================] - 5s 153ms/step - loss: 0.2506 - accuracy: 0.4477 - val_loss: 0.2502 - val_accuracy: 0.4600\n",
      "Epoch 2/4\n",
      "25/25 [==============================] - 4s 144ms/step - loss: 0.2502 - accuracy: 0.4812 - val_loss: 0.2502 - val_accuracy: 0.4800\n",
      "Epoch 3/4\n",
      "25/25 [==============================] - 5s 205ms/step - loss: 0.2498 - accuracy: 0.5107 - val_loss: 0.2503 - val_accuracy: 0.4700\n",
      "Epoch 4/4\n",
      "25/25 [==============================] - 5s 195ms/step - loss: 0.2497 - accuracy: 0.5236 - val_loss: 0.2503 - val_accuracy: 0.4650\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x154897430>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_rnn, y_train_rnn, batch_size=32,\n",
    "          epochs=4, validation_data=(X_test_rnn, y_test_rnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 38ms/step - loss: 0.2503 - accuracy: 0.4650\n"
     ]
    }
   ],
   "source": [
    "score, acc = model.evaluate(X_test_rnn, y_test_rnn, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using TFIDF Vectorizer as input instead of one hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(decode_error='ignore', norm='l2', min_df=5)\n",
    "tfidf_good = vectorizer.fit_transform(filtered_positive_posts)\n",
    "tfidf_bad = vectorizer.fit_transform(filtered_negative_posts)\n",
    "\n",
    "flattened_array_tfidf_good = tfidf_good.toarray()\n",
    "flattened_array_tfidf_bad = tfidf_bad.toarray()\n",
    "\n",
    "#0 bad, 1 good\n",
    "y_rnn = np.concatenate((np.zeros(len(flattened_array_tfidf_bad)),\n",
    "                                        np.ones(len(flattened_array_tfidf_good))))\n",
    "\n",
    "X_train_rnn, X_test_rnn, y_train_rnn, y_test_rnn = train_test_split(np.concatenate((flattened_array_tfidf_male, \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_rnn shape: (800, 149) (800,)\n",
      "X_test_rnn shape: (200, 149) (200,)\n"
     ]
    }
   ],
   "source": [
    "X_train_rnn = sequence.pad_sequences(X_train_rnn, maxlen=maxlen)\n",
    "X_test_rnn = sequence.pad_sequences(X_test_rnn, maxlen=maxlen)\n",
    "print('X_train_rnn shape:', X_train_rnn.shape, y_train_rnn.shape)\n",
    "print('X_test_rnn shape:', X_test_rnn.shape, y_test_rnn.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "25/25 [==============================] - 2s 49ms/step - loss: 0.2501 - accuracy: 0.4914 - val_loss: 0.2498 - val_accuracy: 0.5200\n",
      "Epoch 2/4\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 0.2499 - accuracy: 0.5205 - val_loss: 0.2499 - val_accuracy: 0.5800\n",
      "Epoch 3/4\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.2498 - accuracy: 0.5327 - val_loss: 0.2499 - val_accuracy: 0.5500\n",
      "Epoch 4/4\n",
      "25/25 [==============================] - 1s 36ms/step - loss: 0.2501 - accuracy: 0.4718 - val_loss: 0.2500 - val_accuracy: 0.5250\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.2500 - accuracy: 0.5250\n",
      "0.24996991455554962 0.5249999761581421\n"
     ]
    }
   ],
   "source": [
    "max_features = 30000\n",
    "dimension = 30\n",
    "output_dimension =20\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, dimension))\n",
    "model.add(LSTM(output_dimension))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='mean_squared_error',optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train_rnn, y_train_rnn, \n",
    "          batch_size=32, epochs=4,\n",
    "          validation_data=(X_test_rnn, y_test_rnn))\n",
    "\n",
    "score,acc = model.evaluate(X_test_rnn, y_test_rnn, \n",
    "                           batch_size=32)\n",
    "\n",
    "print(score, acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}